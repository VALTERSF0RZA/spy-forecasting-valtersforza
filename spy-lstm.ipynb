{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, LayerNormalization\nfrom tensorflow.keras.losses import Huber\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom ta.momentum import RSIIndicator\nfrom ta.trend import MACD\nfrom ta.volatility import BollingerBands\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# DETERMINISM\nos.environ['PYTHONHASHSEED'] = '42'\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\nnp.seterr(invalid='ignore')\n\nBASE_PATH = '/kaggle/input/marketdata-ohlcv/market_data'\ncategory_folders = ['mag7', 'sectors', 'crypto', 'forex', 'commodities', 'indices', 'macro']\nall_data = []\ncorrelation_report = []\ngranger_report = []\n\n# TARGET: SPY\nspy_df = pd.read_csv(f'{BASE_PATH}/target/SPY_1d.csv', skiprows=2)\nspy_df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\nspy_df = spy_df[spy_df['Date'] >= '2022-07-01']\nspy_df[['Close']] = spy_df[['Close']].astype(float)\nspy_df['Returns'] = spy_df['Close'].pct_change().bfill()\nspy_returns = spy_df['Returns'].values.reshape(-1, 1)\n\n# TA INDICATORS\ntemp_df = spy_df.copy()\ntemp_df['RSI'] = RSIIndicator(close=temp_df['Close'], window=14).rsi()\ntemp_df['MACD'] = MACD(close=temp_df['Close']).macd()\ntemp_df['MACD_signal'] = MACD(close=temp_df['Close']).macd_signal()\ntemp_df['BB_upper'] = BollingerBands(close=temp_df['Close']).bollinger_hband()\ntemp_df['BB_lower'] = BollingerBands(close=temp_df['Close']).bollinger_lband()\nspy_ta = temp_df[['RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_lower']].values\nspy_close_full = spy_df['Close'].values.reshape(-1, 1)\nspy_dates = pd.to_datetime(spy_df['Date']).reset_index(drop=True).to_numpy()\n\n# LOAD CATEGORY DATA\nfor folder in category_folders:\n    folder_path = os.path.join(BASE_PATH, folder)\n    if not os.path.exists(folder_path):\n        continue\n    for file in os.listdir(folder_path):\n        if file.endswith(\".csv\"):\n            try:\n                df = pd.read_csv(os.path.join(folder_path, file), skiprows=2)\n                df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n                df = df[df['Date'] >= '2022-07-01']\n                for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n                    df[col] = pd.to_numeric(df[col], errors='coerce')\n                df = df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'])\n                df_pct = df[['Open', 'High', 'Low', 'Close', 'Volume']].pct_change()\n                df_pct.replace([np.inf, -np.inf], np.nan, inplace=True)\n                df_pct = df_pct.bfill().ffill()\n\n                spy_len = min(len(df_pct), len(spy_returns))\n                corr = pd.Series(df_pct['Close'][:spy_len]).corr(pd.Series(spy_returns[:spy_len].flatten()))\n                actual_corr = corr if np.isfinite(corr) else None\n                if actual_corr is not None:\n                    if -0.025 <= actual_corr < 0:\n                        weight = 2\n                    elif -0.05 <= actual_corr < -0.025:\n                        weight = 3\n                    elif -0.75 <= actual_corr < -0.05:\n                        weight = 4\n                    elif -0.9 <= actual_corr < -0.75:\n                        weight = 5\n                    elif -1 <= actual_corr < -0.9:\n                        weight = 5\n                    else:\n                        weight = 1\n                    weighted_corr = actual_corr * weight if weight != 1 else actual_corr\n                    correlation_report.append((file, actual_corr, weight, weighted_corr))\n                    if actual_corr < 0:\n                        df_pct['Close'] *= -1\n                    df_pct *= abs(weighted_corr)\n\n                try:\n                    granger_df = pd.DataFrame({\n                        'shock_flag': spy_df['Returns'][:spy_len].values,\n                        'instrument': df_pct['Close'][:spy_len].values\n                    })\n                    granger_result = grangercausalitytests(granger_df[['shock_flag', 'instrument']], maxlag=5, verbose=False)\n                    best_pval = float('inf')\n                    for lag, res in granger_result.items():\n                        pval = res[0]['ssr_ftest'][1]\n                        if pval < best_pval:\n                            best_pval = pval\n                    granger_report.append((file, best_pval))\n                except:\n                    granger_report.append((file, np.nan))\n\n                all_data.append(df_pct.values)\n            except Exception as e:\n                print(f\"[SKIP] {file} due to error: {e}\")\n\n# OUTPUT CORRELATION AND GRANGER REPORTS\ncorrelation_report = sorted(correlation_report, key=lambda x: x[3], reverse=True)\nprint(\"\\n--- Ranked Pearson Correlation of Close vs SPY Returns (Weighted) ---\")\nfor name, corr, mult, w_corr in correlation_report:\n    print(f\"{name:30s} | Corr: {corr: .4f} | Mult: {mult:>3} | Weighted: {w_corr: .4f}\")\n\ngranger_report = sorted(granger_report, key=lambda x: x[1])\nprint(\"\\n--- Ranked Granger Causality p-values (lower is better) ---\")\nfor name, pval in granger_report:\n    print(f\"{name:30s} | p-value: {pval:.4f}\")\n\n# ALIGN LENGTH\nmin_length = min([len(x) for x in all_data] + [len(spy_close_full), len(spy_returns), len(spy_ta)])\nall_data = [x[:min_length] for x in all_data]\nspy_close = spy_close_full[:min_length]\nspy_returns = spy_returns[:min_length]\nspy_dates = spy_dates[:min_length]\nspy_ta = spy_ta[:min_length]\n\n# STACK FEATURES\ndata_combined = np.hstack(all_data + [spy_ta])\ndata_combined = np.nan_to_num(data_combined)\nspy_close = np.nan_to_num(spy_close)\n\n# SCALE\nfeature_scaler = StandardScaler()\nscaled_data = feature_scaler.fit_transform(data_combined)\nspy_scaler = StandardScaler()\nspy_scaled = spy_scaler.fit_transform(spy_close)\n\n# SUPERVISED LEARNING SETUP\nn_lookback = 252\nn_forecast = 30\nX, y = [], []\nfor i in range(n_lookback, len(scaled_data) - n_forecast + 1):\n    X.append(scaled_data[i - n_lookback:i])\n    y.append(spy_scaled[i + n_forecast - 1, 0] - spy_scaled[i - 1, 0])\n\nX, y = np.array(X), np.array(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# MODEL\nmodel = Sequential([\n    Input(shape=(X.shape[1], X.shape[2])),\n    Bidirectional(LSTM(128, return_sequences=True)),\n    LayerNormalization(),\n    Dropout(0.5),\n    LSTM(128),\n    LayerNormalization(),\n    Dropout(0.5),\n    Dense(1)\n])\nmodel.compile(optimizer='adam', loss=Huber(delta=1.0))\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n]\nhistory = model.fit(X_train, y_train, epochs=300, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks)\n\n# PLOT LOSS\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# BACKTEST\npred_test = model.predict(X_test).flatten()\npred_test_cumsum = spy_scaled[n_lookback + len(X_train) - 1:n_lookback + len(X_train) - 1 + len(pred_test), 0] + pred_test\npred_test_inv = spy_scaler.inverse_transform(pred_test_cumsum.reshape(-1, 1))\n\nactual_test = spy_scaled[n_lookback + len(X_train):n_lookback + len(X_train) + len(pred_test), 0]\ny_test_inv = spy_scaler.inverse_transform(actual_test.reshape(-1, 1))\n\ncorr_test = np.corrcoef(pred_test_inv[:, 0], y_test_inv[:, 0])[0, 1]\nprint(f\"[INFO] Correlation between actual SPY and predicted SPY: {corr_test:.4f}\")\n\ntest_dates = pd.Series(spy_dates[n_lookback + len(X_train):n_lookback + len(X_train) + len(pred_test)]).reset_index(drop=True)\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=test_dates, y=y_test_inv[:, 0], name='Actual SPY (Day 90)', line=dict(color='green')))\nfig.add_trace(go.Scatter(x=test_dates, y=pred_test_inv[:, 0], name='Predicted SPY (Day 90)', line=dict(color='blue', dash='dash')))\n\n# FUTURE FORECAST\nforecast_range = 90\nrolling_preds = []\nrolling_dates = []\n\nfor i in range(-90, 0):  # last 90 days of data\n    window_start = i - n_lookback\n    window_end = i\n    if abs(window_start) > len(scaled_data):\n        continue\n    window = scaled_data[window_start:window_end]\n    if window.shape[0] == n_lookback:\n        pred = model.predict(window.reshape(1, n_lookback, X.shape[2])).flatten()[0]\n        last_price_scaled = spy_scaled[i - 1, 0] + pred\n        future_price = spy_scaler.inverse_transform([[last_price_scaled]])[0, 0]\n        rolling_preds.append(future_price)\n        rolling_dates.append(spy_dates[i] + pd.Timedelta(days=1))\n\nfig.add_trace(go.Scatter(x=rolling_dates, y=rolling_preds, name='Rolling 90d Forecast', line=dict(color='orange', dash='dot')))\n\n# Static future prediction from latest window\nlast_window = scaled_data[-n_lookback:].reshape(1, n_lookback, X.shape[2])\nfuture_delta = model.predict(last_window).flatten()[0]\nlast_price_scaled = spy_scaled[-1, 0] + future_delta\nfuture_price = spy_scaler.inverse_transform([[last_price_scaled]])[0, 0]\nfuture_date = spy_dates[-1] + pd.Timedelta(days=1)\nfig.add_trace(go.Scatter(x=[future_date], y=[future_price], name='Future Predicted SPY', mode='markers+text', marker=dict(color='orange', size=10), text=['Prediction'], textposition='top center'))\n\nfig.update_layout(title='LSTM SPY Forecast (Weighted Pearson + Granger)', xaxis_title='Date', yaxis_title='SPY Close Price', template='plotly_dark')\nfig.show()\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}